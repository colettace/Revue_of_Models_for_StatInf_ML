{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop title: A \"Revue\" of Models for Statistical Inference and Machine Learning\n",
    "\n",
    "<img style=\"align: left\" src=\"./A-Chorus-Line-541x346.jpg\" />\n",
    "\n",
    "# Workshop description\n",
    "* A high-level overview of common models used for inference (linear, generalized linear, generalized linear mixed, LASSO, ElasticNet) and prediction (random forests, gradient boosted trees, neural networks).\n",
    "* Intended use case, deployment strategies, advantages and common pitfalls for each will be discussed.\n",
    "* Example code for all models provided in both R and Python for quick adaptation to your project.\n",
    "* From known parameters, we will create synthetic data with ever-more exotic variance structures (non Gaussian-distributed, non i.i.d., heteroscedastic data), visualize the data, and use appropriate models to back out the parameters we used to make the data.\n",
    "* Considerations including preprocessing, interpretation, diagnostics, model selection, outliers, overdispersion, and corrections for multiple comparisons will be discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "* Must have taken an Intro to Stats course at some time in your life.\n",
    "* Must have run some R or Python code of your own accord at some time in your life.\n",
    "* Must know what a data frame is and what it's used for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation: Poohsticks game\n",
    "\n",
    "<img style=\"align: left\" src=\"./poohsticks.jpg\" />\n",
    "\n",
    "* Idea: input data with known properties into various models, and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop outline\n",
    "\n",
    "1. Day 1. Introduction; LM and problem of multicolinearity; LASSO; R leaps package\n",
    "2. Day 2. Data distributions for dependent (outcome) variables and independent (predictor) variables; GLM (Logistic & Poisson); survival analysis\n",
    "3. Day 3. Linear Mixed Effect Models\n",
    "4. Day 4. Models for machine learning: Random Forests, XGBoost, Neural Networks; AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1 outline\n",
    "\n",
    "1. Personal introductions\n",
    "2. Modelling caveats\n",
    "3. Difference between statistical inference and machine learning\n",
    "4. Problem of Multicolinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# About the Computational Biology Genomics Core (CBGC)\n",
    "\n",
    "<img style=\"align: right; float: right;\" alt=\"Computational Biology Core logo\" src=\"./CBClogo_200.png\"/>\n",
    "<ul>\n",
    "<li>Core facility housed in LGG</li>\n",
    "<li>Room 10C222</li>\n",
    "<li>Seminar or training every month</li>\n",
    "<li>Two powerful Windows computers with lots of software and remote access available</li>\n",
    "<li>BRC cloud computing (RAM, GPUs, virtual machines)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CBGC Staff\n",
    "    \n",
    "* Supriyo De, Ph.D., Head\n",
    "* Elin Lehrmann, Ph.D., Biologist\n",
    "* Jinshui Fan, Ph.D, Biologist\n",
    "* Yongqing Zhang, Ph.D., Computer Scientist\n",
    "* Gabriel Lam, Ph.D, Computational Biologist\n",
    "* Nirad Banskota, M.S., Computational Biologist\n",
    "* Christopher Coletta, M.S., Computer Scientist\n",
    "* Qiong (Joan) Meng, Ph.D., Post-doctoral fellow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling Caveats\n",
    "\n",
    "## \"All models are wrong, but some are useful.\"\n",
    "\n",
    "* Sir David Cox, originator of the Cox proportional hazards model, said: The idea that complex physical, biological or sociological systems can be exactly described by a few formulae is patently absurd.\"\n",
    "* Statistician George Box said: \"Cunningly chosen parsimonious models often do provide remarkably useful approximations.\" He then cites the ideal gas law pv=nRT as an example. \"For such a model there is no need to ask the question 'Is the model true?'. If 'truth' is to be the 'whole truth' the answer must be 'No'. The only question of interest is 'Is the model illuminating and useful?'.\"\n",
    "\n",
    "## Corellation does not imply causation\n",
    "* Causal inference = correlation, plus causal reasonong, which involves the \"ceteris paribus\" assmption- \"The only difference is what we changed.\"\n",
    "* Otherwise reporting correlation is the best we can do.\n",
    "\n",
    "## Weapons of Math Destruction\n",
    "\n",
    "<img style=\"align: left\" src=\"https://upload.wikimedia.org/wikipedia/en/0/0b/Weapons_of_Math_Destruction.jpg\" />\n",
    "\n",
    "* Evil : Measuring proxies rather than measuring the actual thing\n",
    "    * Less evil: Credit score - it is a measure of how likely a person is to default on a loan. Have you defaulted before? There is redress to fix things if there are discrepancies.\n",
    "    * More evil: US News and World Report college rankings\n",
    "    * When a measure becomes a target, it ceases to be a good measure.\n",
    "* Evil: Predictive models that use past outcomes as training set, TO PERPETUATE future outcomes\n",
    "    * Algorithms that set bailbonds amounts\n",
    "* Cognitive bias in Machine learning is human bias on steroids\n",
    "    * We seek out evidence that supports our existing point of view while avoiding information that contradicts it\n",
    "\n",
    "## Checking for multiple comparisons\n",
    "\n",
    "* Come to Osorio Meirelles's workshop!\n",
    "* The goal of adjusting for multiple comparisons is to reduce the number of false positives.\n",
    "* Upshot: Every single individual p-value you get, including all the pvalues for the betas in a single model, is a (potential) target to be adjusted for multiple comparisons.\n",
    "    * Bonferroni: \"Family-wise error rate\"; too strict\n",
    "    * Benjamini-Hochburg: \"False discovery rate\"; less strict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference between statistical inference and machine learning\n",
    "\n",
    "* https://www.coursera.org/lecture/statistical-genomics/inference-vs-prediction-8-52-PkWHh\n",
    "* Model interpretability versus predictive power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Pipeline\n",
    "\n",
    "* Pre-processing, a.k.a. \"data wrangling\"\n",
    "\n",
    "## SEMMA\n",
    "### Sample\n",
    "* Import the data\n",
    "* Check the data types\n",
    "* For predictive modeling only: partition into training & validation sets\n",
    "    * What is the sampling unit? Read? Person?\n",
    "\n",
    "### Explore\n",
    "* Look for outliers\n",
    "* Univariate descriptive statistics\n",
    "* Bivariate descriptive statistics - include target variable\n",
    "* Cluster - a descriptive model\n",
    "* Check for multicolinearity\n",
    "\n",
    "### Modify\n",
    "* Transform\n",
    "* Impute\n",
    "* Replacement\n",
    "* Drop\n",
    "\n",
    "### Model (with metrics)\n",
    "* Linear regression (Adjusted-$R^2$, p-values)\n",
    "* Logistic regression (accuracy, p-values)\n",
    "* Random forests, gradient-boosted trees, neural networks (accuracy)\n",
    "\n",
    "### Assess\n",
    "* Model comparison for predictive model\n",
    "* Visualize\n",
    "* Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
